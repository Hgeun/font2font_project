{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.font_dataset at 0x2570726bf28>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class font_dataset(Dataset):\n",
    "    def __init__(self, data_paths):\n",
    "        self.path1 = './dataset/input/misaeng/'\n",
    "        self.path2 = './dataset/target/yeonsung/'\n",
    "\n",
    "    def __getitem(self, index):\n",
    "        image = load_images(self.path1[index])\n",
    "        target_image = load_images(self.path2[index]) # load as np.array or PIL.Image\n",
    "        # transform your images here\n",
    "        x = transformations.ToTensor()(image)\n",
    "        y = transformations.ToTensor()(target_image)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path1) \n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "\n",
    "input_data = font_dataset('./dataset/')\n",
    "#target_data = torchvision.datasets.ImageFolder('./dataset/target/',transform = transform)\n",
    "\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(input_data, batch_size=10,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_loader = torch.utils.data.DataLoader(target_data, batch_size=10,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader._DataLoaderIter"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataiter = iter(data_loader)\n",
    "#type(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, 1, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, 1, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 3, 5, 1, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(3)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(16, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn3): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-01f3dd481e24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         '''\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # 데이터셋을 수차례 반복합니다.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i,data in :\n",
    "        print(i,data)\n",
    "        '''\n",
    "        # 입력을 받은 후\n",
    "        inputs,target = data\n",
    "        print(inputs)\n",
    "        print(target)\n",
    "        #inputs torch.Size([4, 3, 32, 32])\n",
    "        # 변화도(Gradient) 매개변수를 0으로 만든 후\n",
    "        optimizer.zero_grad()\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "        # 순전파 + 역전파 + 최적화\n",
    "        outputs = net(inputs)\n",
    "        print(outputs)\n",
    "        print(inputs.shape, target.shape, outputs.shape)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 통계 출력\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 499:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 500))\n",
    "            running_loss = 0.0\n",
    "        '''\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = np.genfromtxt('trainset.txt',dtype=None,encoding='UTF-8')\n",
    "list2 = np.genfromtxt('testset.txt',dtype=None,encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['D:\\\\03_code\\\\18_2_image_processing\\\\fontConvNet-master\\\\misaeng\\\\44032.jpg',\n",
       "       'D:\\\\03_code\\\\18_2_image_processing\\\\fontConvNet-master\\\\yeonsung\\\\44032.jpg'],\n",
       "      dtype='<U70')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\03_code\\\\18_2_image_processing\\\\fontConvNet-master\\\\misaeng\\\\44032.jpg'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2570895e0b8>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import PIL.Image\n",
    "\n",
    "img = PIL.Image.open(list1[0][0])\n",
    "trans = transforms.ToPILImage()\n",
    "trans1 = transforms.ToTensor()\n",
    "plt.imshow(trans(trans1(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path='D:\\03_code\\18_2_image_processing\\fontConvNet-master\\misaeng\\44040.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\x03_code\\x018_2_image_processing\\x0contConvNet-master\\\\misaengĠ40.jpg'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ngĠ40.jpg'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path[-9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "[1,   100] loss: 0.588\n",
      "[1,   200] loss: 0.428\n",
      "[1,   300] loss: 0.353\n",
      "[1,   400] loss: 0.260\n",
      "[1,   500] loss: 0.222\n",
      "[1,   600] loss: 0.208\n",
      "[1,   700] loss: 0.206\n",
      "[1,   800] loss: 0.206\n",
      "[1,   900] loss: 0.189\n",
      "[1,  1000] loss: 0.177\n",
      "[1,  1100] loss: 0.177\n",
      "[1,  1200] loss: 0.195\n",
      "[1,  1300] loss: 0.174\n",
      "[1,  1400] loss: 0.170\n",
      "[1,  1500] loss: 0.190\n",
      "[1,  1600] loss: 0.174\n",
      "[1,  1700] loss: 0.166\n",
      "[1,  1800] loss: 0.170\n",
      "[1,  1900] loss: 0.171\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, 1, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, 1, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 3, 5, 1, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net.to(device)\n",
    "\n",
    "train_list = np.genfromtxt('./trainset.txt',dtype=None,encoding='UTF-8')\n",
    "test_list = np.genfromtxt('./testset.txt',dtype=None,encoding='UTF-8')\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "trans_img = transforms.ToPILImage()\n",
    "\n",
    "for epoch in range(1):  # 데이터셋을 수차례 반복합니다.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i,(input_path,target_path) in enumerate(train_list):\n",
    "        input = PIL.Image.open(input_path)\n",
    "        target = PIL.Image.open(target_path)\n",
    "        input=trans(input).reshape(1,3,64,64)\n",
    "        #print(input.shape)\n",
    "        target=trans(target).reshape(1,3,64,64)\n",
    "        optimizer.zero_grad()\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        #print(input_path)print(target_path)\n",
    "        output = net(input)\n",
    "        #ce_loss = torch.mean(torch.pow(output-target,2))\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9919, 0.9921, 0.9394,  ..., 0.9747, 1.0115, 1.0445],\n",
       "          [0.9920, 0.9726, 0.9947,  ..., 0.9857, 1.0288, 1.0230],\n",
       "          [0.9634, 0.9559, 0.9562,  ..., 1.0530, 1.0580, 1.0457],\n",
       "          ...,\n",
       "          [1.0579, 1.0325, 1.0172,  ..., 0.9717, 0.9781, 1.0004],\n",
       "          [0.9775, 0.9995, 1.0522,  ..., 0.9572, 0.9884, 0.9224],\n",
       "          [0.9863, 0.9266, 0.9961,  ..., 0.9664, 0.9323, 0.9292]],\n",
       "\n",
       "         [[0.9946, 1.0318, 1.0105,  ..., 1.0438, 1.0278, 1.0123],\n",
       "          [1.0317, 0.9832, 0.9270,  ..., 1.0284, 1.0255, 1.0289],\n",
       "          [0.9433, 0.9643, 0.9709,  ..., 1.0083, 0.9850, 1.0305],\n",
       "          ...,\n",
       "          [1.0061, 1.0458, 1.0412,  ..., 0.9600, 0.9634, 0.9676],\n",
       "          [1.0041, 1.0249, 1.0289,  ..., 0.9600, 0.9886, 0.9746],\n",
       "          [0.9808, 0.9770, 0.9725,  ..., 0.9635, 0.9363, 0.9521]],\n",
       "\n",
       "         [[0.9886, 0.9752, 0.9812,  ..., 1.0142, 1.0222, 0.9801],\n",
       "          [0.9525, 0.9704, 0.9625,  ..., 0.9808, 1.0019, 1.0134],\n",
       "          [0.9921, 0.9679, 0.9364,  ..., 1.0435, 1.0002, 0.9989],\n",
       "          ...,\n",
       "          [1.0267, 0.9873, 1.0027,  ..., 0.9783, 0.9660, 1.0012],\n",
       "          [1.0195, 0.9810, 0.9739,  ..., 0.9805, 0.9581, 0.9985],\n",
       "          [1.0148, 0.9895, 0.9807,  ..., 0.9791, 0.9794, 0.9894]]]],\n",
       "       device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = output.reshape(3,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-7680251fddde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "img = np.array(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToPILImage(mode=<built-in method int of Tensor object at 0x00000237105399D8>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-23af6c37b3d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2699\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2700\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[1;32m-> 2701\u001b[1;33m         None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2702\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2703\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1810\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5494\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    640\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[0;32m    641\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m--> 642\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Image data cannot be converted to float\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         if not (self._A.ndim == 2\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9de497cea54e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrans_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mpic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mnpimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "img=trans_img(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i,(input_path,target_path) in enumerate(test_list):\n",
    "        input = PIL.Image.open(input_path)\n",
    "        target = PIL.Image.open(target_path)\n",
    "        input = trans(input).reshape(1, 3, 64, 64)\n",
    "        target = trans(target).reshape(1, 3, 64, 64)\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        output = net(input)\n",
    "        \n",
    "        print(output.shape)\n",
    "        print(output)\n",
    "    \n",
    "        loss = criterion(output, target)\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        img=trans_img(output)\n",
    "        plt.imsave('../output/'+input_path[-9:],img)\n",
    "\n",
    "print('Finished Testing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.584\n",
      "[1,   200] loss: 0.410\n",
      "[1,   300] loss: 0.340\n",
      "[1,   400] loss: 0.265\n",
      "[1,   500] loss: 0.235\n",
      "[1,   600] loss: 0.210\n",
      "[1,   700] loss: 0.209\n",
      "[1,   800] loss: 0.210\n",
      "[1,   900] loss: 0.184\n",
      "[1,  1000] loss: 0.177\n",
      "[1,  1100] loss: 0.174\n",
      "[1,  1200] loss: 0.196\n",
      "[1,  1300] loss: 0.178\n",
      "[1,  1400] loss: 0.175\n",
      "[1,  1500] loss: 0.187\n",
      "[1,  1600] loss: 0.177\n",
      "[1,  1700] loss: 0.169\n",
      "[1,  1800] loss: 0.171\n",
      "[1,  1900] loss: 0.173\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, 1, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, 1, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 3, 5, 1, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "#net.to(device)\n",
    "\n",
    "train_list = np.genfromtxt('./trainset.txt',dtype=None,encoding='UTF-8')\n",
    "test_list = np.genfromtxt('./testset.txt',dtype=None,encoding='UTF-8')\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "trans_img = transforms.ToPILImage()\n",
    "\n",
    "for epoch in range(1):  # 데이터셋을 수차례 반복합니다.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i,(input_path,target_path) in enumerate(train_list):\n",
    "        input = PIL.Image.open(input_path)\n",
    "        target = PIL.Image.open(target_path)\n",
    "        input=trans(input).reshape(1,3,64,64)\n",
    "        #print(input.shape)\n",
    "        target=trans(target).reshape(1,3,64,64)\n",
    "        optimizer.zero_grad()\n",
    "        #input, target = input.to(device), target.to(device)\n",
    "        #print(input_path)print(target_path)\n",
    "        output = net(input)\n",
    "        #ce_loss = torch.mean(torch.pow(output-target,2))\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0169, 0.9821, 1.0607,  ..., 0.9945, 0.9982, 1.0225],\n",
       "          [0.9567, 0.9433, 1.0033,  ..., 0.9776, 0.9770, 0.9759],\n",
       "          [0.9945, 0.9431, 0.9405,  ..., 0.9783, 0.9770, 0.9865],\n",
       "          ...,\n",
       "          [1.0370, 0.9408, 0.9482,  ..., 1.0202, 1.0025, 1.0024],\n",
       "          [0.9926, 0.8905, 0.9082,  ..., 1.0143, 0.9780, 0.9376],\n",
       "          [0.9939, 0.8723, 0.7906,  ..., 0.9698, 0.9937, 0.9835]],\n",
       "\n",
       "         [[1.0291, 1.0325, 1.0556,  ..., 0.9910, 0.9494, 1.0283],\n",
       "          [0.9569, 0.9526, 0.9818,  ..., 0.9736, 0.9404, 0.9716],\n",
       "          [0.9841, 1.0343, 0.9470,  ..., 0.9898, 0.9559, 1.0044],\n",
       "          ...,\n",
       "          [0.9893, 0.9755, 0.8527,  ..., 1.0304, 1.0258, 1.0310],\n",
       "          [0.9605, 0.9465, 0.8025,  ..., 0.9778, 1.0190, 0.9342],\n",
       "          [1.0416, 0.9528, 0.8353,  ..., 0.9967, 0.9822, 1.0103]],\n",
       "\n",
       "         [[0.9972, 0.9902, 1.0216,  ..., 1.0161, 0.9855, 0.9865],\n",
       "          [0.9913, 1.0145, 1.0120,  ..., 0.9942, 0.9865, 0.9837],\n",
       "          [1.0003, 1.0304, 0.9984,  ..., 1.0062, 0.9939, 0.9930],\n",
       "          ...,\n",
       "          [1.0241, 0.9834, 0.9965,  ..., 0.9949, 0.9825, 0.9783],\n",
       "          [1.0363, 0.9978, 0.9812,  ..., 0.9794, 0.9969, 1.0180],\n",
       "          [1.0279, 1.0083, 0.9601,  ..., 0.9804, 1.0076, 1.0023]]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1=output.data\n",
    "output1 = output1.reshape(3,64,64)*255\n",
    "output1[output1>255] =255\n",
    "trans_img(output1)\n",
    "output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[255.0000, 250.4348, 255.0000,  ..., 253.6009, 254.5450, 255.0000],\n",
       "         [243.9568, 240.5330, 255.0000,  ..., 249.2775, 249.1464, 248.8571],\n",
       "         [253.6093, 240.4852, 239.8260,  ..., 249.4596, 249.1252, 251.5448],\n",
       "         ...,\n",
       "         [255.0000, 239.8914, 241.7927,  ..., 255.0000, 255.0000, 255.0000],\n",
       "         [253.1230, 227.0705, 231.5969,  ..., 255.0000, 249.4007, 239.0962],\n",
       "         [253.4412, 222.4280, 201.6142,  ..., 247.2908, 253.3913, 250.8009]],\n",
       "\n",
       "        [[255.0000, 255.0000, 255.0000,  ..., 252.7037, 242.1034, 255.0000],\n",
       "         [244.0056, 242.9254, 250.3654,  ..., 248.2558, 239.8100, 247.7681],\n",
       "         [250.9332, 255.0000, 241.4903,  ..., 252.4093, 243.7475, 255.0000],\n",
       "         ...,\n",
       "         [252.2713, 248.7402, 217.4374,  ..., 255.0000, 255.0000, 255.0000],\n",
       "         [244.9184, 241.3662, 204.6486,  ..., 249.3287, 255.0000, 238.2213],\n",
       "         [255.0000, 242.9573, 213.0001,  ..., 254.1533, 250.4731, 255.0000]],\n",
       "\n",
       "        [[254.2779, 252.4935, 255.0000,  ..., 255.0000, 251.3139, 251.5642],\n",
       "         [252.7921, 255.0000, 255.0000,  ..., 253.5098, 251.5582, 250.8528],\n",
       "         [255.0000, 255.0000, 254.5948,  ..., 255.0000, 253.4535, 253.2204],\n",
       "         ...,\n",
       "         [255.0000, 250.7723, 254.1124,  ..., 253.7094, 250.5275, 249.4649],\n",
       "         [255.0000, 254.4383, 250.2185,  ..., 249.7449, 254.2091, 255.0000],\n",
       "         [255.0000, 255.0000, 244.8290,  ..., 249.9962, 255.0000, 255.0000]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = output1/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[247.4131, 255.0000, 255.0000,  ..., 249.2345, 250.5160, 255.0000],\n",
       "         [246.5345, 254.5781, 248.5174,  ..., 254.6129, 250.1243, 252.0640],\n",
       "         [253.8601, 255.0000, 250.0230,  ..., 255.0000, 255.0000, 253.3613],\n",
       "         ...,\n",
       "         [255.0000, 255.0000, 254.1543,  ..., 253.9975, 253.8753, 255.0000],\n",
       "         [255.0000, 245.7914, 255.0000,  ..., 254.3198, 249.7484, 255.0000],\n",
       "         [255.0000, 247.6770, 254.6254,  ..., 255.0000, 255.0000, 255.0000]],\n",
       "\n",
       "        [[245.2520, 255.0000, 254.6374,  ..., 255.0000, 254.4605, 255.0000],\n",
       "         [243.6316, 250.6969, 255.0000,  ..., 255.0000, 255.0000, 253.6809],\n",
       "         [255.0000, 253.5595, 255.0000,  ..., 255.0000, 255.0000, 253.7761],\n",
       "         ...,\n",
       "         [252.0380, 254.4707, 255.0000,  ..., 255.0000, 255.0000, 255.0000],\n",
       "         [255.0000, 251.3991, 255.0000,  ..., 252.8454, 251.7261, 251.3331],\n",
       "         [255.0000, 251.4628, 236.6734,  ..., 255.0000, 255.0000, 250.6402]],\n",
       "\n",
       "        [[236.4759, 255.0000, 255.0000,  ..., 252.9038, 253.5053, 255.0000],\n",
       "         [255.0000, 255.0000, 255.0000,  ..., 252.1207, 246.4295, 255.0000],\n",
       "         [250.4867, 255.0000, 250.9156,  ..., 251.3234, 250.9879, 242.4720],\n",
       "         ...,\n",
       "         [251.1240, 255.0000, 255.0000,  ..., 254.8770, 250.5127, 255.0000],\n",
       "         [255.0000, 250.1463, 242.5965,  ..., 248.5314, 255.0000, 255.0000],\n",
       "         [249.8734, 239.6648, 255.0000,  ..., 251.8636, 255.0000, 254.5510]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=trans_img(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='./output/'+input_path[-9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-2bb919819881>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-38-2bb919819881>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    PIL.Image.(filename,img)\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "PIL.Image.(filename,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_=(output.data*255).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[259.31018, 250.4348 , 270.469  , ..., 253.60089, 254.54503,\n",
       "          260.72803],\n",
       "         [243.9568 , 240.53296, 255.84634, ..., 249.27751, 249.14638,\n",
       "          248.8571 ],\n",
       "         [253.60928, 240.48523, 239.826  , ..., 249.4596 , 249.12515,\n",
       "          251.54478],\n",
       "         ...,\n",
       "         [264.43408, 239.89145, 241.79266, ..., 260.15442, 255.6365 ,\n",
       "          255.61642],\n",
       "         [253.12296, 227.07053, 231.59691, ..., 258.64545, 249.40068,\n",
       "          239.09618],\n",
       "         [253.4412 , 222.42804, 201.61423, ..., 247.29085, 253.39125,\n",
       "          250.80095]],\n",
       "\n",
       "        [[262.42728, 263.2781 , 269.16748, ..., 252.70366, 242.10342,\n",
       "          262.20914],\n",
       "         [244.00558, 242.9254 , 250.36537, ..., 248.25581, 239.80997,\n",
       "          247.76811],\n",
       "         [250.93318, 263.7424 , 241.49028, ..., 252.40927, 243.74751,\n",
       "          256.1229 ],\n",
       "         ...,\n",
       "         [252.27129, 248.74023, 217.4374 , ..., 262.7573 , 261.58847,\n",
       "          262.91452],\n",
       "         [244.91841, 241.3662 , 204.64856, ..., 249.32867, 259.85092,\n",
       "          238.22127],\n",
       "         [265.6034 , 242.95732, 213.00006, ..., 254.15332, 250.47313,\n",
       "          257.62836]],\n",
       "\n",
       "        [[254.27792, 252.49348, 260.49997, ..., 259.09558, 251.3139 ,\n",
       "          251.56416],\n",
       "         [252.79213, 258.69934, 258.04892, ..., 253.50983, 251.55817,\n",
       "          250.85277],\n",
       "         [255.0754 , 262.7584 , 254.59477, ..., 256.58136, 253.45354,\n",
       "          253.2204 ],\n",
       "         ...,\n",
       "         [261.14035, 250.77226, 254.1124 , ..., 253.70935, 250.52753,\n",
       "          249.46489],\n",
       "         [264.24634, 254.43832, 250.21852, ..., 249.74486, 254.20914,\n",
       "          259.59875],\n",
       "         [262.11188, 257.11404, 244.82896, ..., 249.99622, 256.93646,\n",
       "          255.58864]]]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=output_.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  3, 250,  14, ..., 253, 254,   4],\n",
       "        [243, 240, 255, ..., 249, 249, 248],\n",
       "        [253, 240, 239, ..., 249, 249, 251],\n",
       "        ...,\n",
       "        [  8, 239, 241, ...,   4, 255, 255],\n",
       "        [253, 227, 231, ...,   2, 249, 239],\n",
       "        [253, 222, 201, ..., 247, 253, 250]],\n",
       "\n",
       "       [[  6,   7,  13, ..., 252, 242,   6],\n",
       "        [244, 242, 250, ..., 248, 239, 247],\n",
       "        [250,   7, 241, ..., 252, 243,   0],\n",
       "        ...,\n",
       "        [252, 248, 217, ...,   6,   5,   6],\n",
       "        [244, 241, 204, ..., 249,   3, 238],\n",
       "        [  9, 242, 213, ..., 254, 250,   1]],\n",
       "\n",
       "       [[254, 252,   4, ...,   3, 251, 251],\n",
       "        [252,   2,   2, ..., 253, 251, 250],\n",
       "        [255,   6, 254, ...,   0, 253, 253],\n",
       "        ...,\n",
       "        [  5, 250, 254, ..., 253, 250, 249],\n",
       "        [  8, 254, 250, ..., 249, 254,   3],\n",
       "        [  6,   1, 244, ..., 249,   0, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.reshape(3,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 64, 64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.ToTensor()\n",
    "trans_img = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n",
      "(3, 64, 64)\n",
      "(3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "output1 = output.data #output1 tensor\n",
    "print(output1.shape)\n",
    "output1 = (output1.reshape(3, 64, 64) * 255).numpy()\n",
    "print(output1.shape)\n",
    "output1 = output1.astype(np.uint8)\n",
    "print(output1.shape)\n",
    "#output1 = trans(output1)\n",
    "#print(output1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi =(output1[0]+output1[1]+output1[2])/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22421e90048>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = np.mean(output1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(filename,mean1,cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-32ad6cb7aa56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Image' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 64])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./1222_18_12_50.pth'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'./' + datetime.today().strftime(\"%m%d_%H_%M_%S\") +'.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2489\u001b[0m             \u001b[0mtypekey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'typestr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2490\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2491\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 64), '|u1')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-abae9edf8d27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2490\u001b[0m             \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2492\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot handle this data type\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2493\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2494\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type"
     ]
    }
   ],
   "source": [
    "img = PIL.Image.fromarray(aa.reshape(3,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave('./output/'+input_path[-9:],img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-acc24acb55be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2699\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2700\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[1;32m-> 2701\u001b[1;33m         None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2702\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2703\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1810\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5494\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    644\u001b[0m         if not (self._A.ndim == 2\n\u001b[0;32m    645\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m--> 646\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'numpy.ndarray'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-05b676716c0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrans_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \"\"\"\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_is_numpy_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_tensor_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pic should be Tensor or ndarray. Got {}.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mnpimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'numpy.ndarray'>."
     ]
    }
   ],
   "source": [
    "img = trans_img(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = PIL.Image.open(input_path).convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAFVklEQVR4nO2XS6wlVRWG/12nqs7rvujb9EVsedhKY4iJGBmAE6MTjFw00cRWEhzq0JA4MdGkTYw4MTggkhglgtoxvgJGO2EEMUJ3xA4gxkDUYAvdtLe5j1On6lTVfnwODpy+3qr7SJg4cI1OTvb+6l97rb3W2ga9PYve5v7/A/43AaWkdUmykpeKWpdlJ0ESkuQbBHaYP7H6B6BmAmQBAiOoHcEHIOxc3wA8pL5WX8nJ4DKQbTrqnAy88zvXtgKelt6h4fdL8FvUFbAOJUBofL0VMF6VlOrjz46xFB6Cdc98/am2ve0A8o8Oj6TSsUchh6qwcKOOv3BgQIW7X7pauvZrDjaBEUmqeya23YsGIGDDxgmjBS0/OWGL0sKqFrVZ+JYY/BdgCxiDJ8Avjpq+vjEBanJOKdZ3qD2u3B1QUG+NwFHBOHDhy8duOsc4A+d4No10L963+TADeByWqQhqyMavjScEmEB5vTrvhlCC2wNQjW9L4rmVuUGqNJWULmj1lzUWx12KlDkqaCTT9kMspb6UxFKyEA8GUqr4ZxWh5KSkv5ct2+HKZZrk6fHESjbEHTtyRdE97OTOOHnp/VJ0XlJoluAZwCZD81njhrGCUyeeT5ary32tvC9Wp6NrY+mfaRCmAYjf+pHIRic/+NzYvGtl6cjhxfk6LUy9KAnT0SDx4bKpukahcf9nzrgKmxMYUVkqD2FMVgIhON+VuY+seQIwUyCTyHTlkvmNQfTHC8tHbyj7mlM+MJIxRiqUtHhwRUHwhIBzZK/dsySj+M4NHGU1rSJDo8+1BmFbGC0BasIpzWlRXSM9GqjeTP+5SHc7vNurIlUBcPxcPZl5SQsLeqkmmwHusgS7lwJfETx/7SUD3fytf73yq6u1eITAzIXP+FYftqVyQQ0PqK+bHG8wuvhOzT23SSAEH3rSF3DQlDALazWt2WeSifmd3zhUzV9zs6qzi9ZMD1rqyUlh90zsyyhVdtTIHEu7MtIldf7irEamYGQVz6kblOwOQB1Jg1tq9X9TDpTq4fMqo3igvoZxFhQNpLY0mCWSTSMZdW7v1/WJ2+/Q6MxZRdGHladm3NOauiy17N6eSCU+4PAnu1In6i9KPX3kQgBKwlOKox+2d4YZwGItDviiuv2e1FHyiXPUVUbleaIjPeZ92CuM4BzBkvP4p7vS4tynTllGOVhK+0gsPe1cs6BtBwQA7yd4wusvXiwAKkpeBe6X9DKhDXDlejvJK1Ivj+SXb7kmUpXLKH34uxesSg2T+dYgNNv7zCxk1A8q+STh3Iq+tEG1913YYUXNJvbHS5LW1uH8RbItsPvPB9sQ2NNDRUs3wOYIyPy03R8Y4Dk7UEeH/sbrwJYD5w8wobxlecWfD0u68Xksb1DjKXNoSNhdQbj0ASmOHx9BUXk/bUvNVNp1zPPc+fxAyUI9n9tOGql2UhTGzfW7CbCf72vY05xu/WnxZh3JKvZo7zPlhAxyvmLUk1ZkpA89uI5n0v6hBsDVjrLmAUXS8VN3DKVY5taHcstaEXD7H2LFGBd+EiVmYelPzj5z97QOvOeb/wDGB3DBV9ST05LR4pNbAC/cd11nPpF07xOwtj/AssbvtTCv5dOAy3zh//3tZR2KI5nrf029L4B1wnuVaOV7EzeqAOqKyY8+JnWSpD/eX4ErnFtK1fkBwBa46VxUvPjV29S9jmJ/BUx4RFc9ZnE51NNCaD1Mqld/e6lZFM3OocWbSMUoueqgL5F45x+RN6ZvogO/ZBoKZEkPullqezNFRlJoNsEDKxAGrwP70ACESNrjlu8PaB2k9rDGp8z0fdd83u1i/wFUWGcRH7icMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x237219D7D68>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, 1, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 10, 5, 1, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(10)\n",
    "        self.conv3 = nn.Conv2d(10, 1, 5, 1, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "#net.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 1 elements not 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7eaed9724e56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#input, target = input.to(device), target.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#print(input_path)print(target_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;31m#ce_loss = torch.mean(torch.pow(output-target,2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-ef52d0da7a29>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1621\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1622\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1623\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1624\u001b[0m     )\n\u001b[0;32m   1625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 1 elements not 3"
     ]
    }
   ],
   "source": [
    "train_list = np.genfromtxt('./trainset.txt',dtype=None,encoding='UTF-8')\n",
    "test_list = np.genfromtxt('./testset.txt',dtype=None,encoding='UTF-8')\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "trans_img = transforms.ToPILImage()\n",
    "\n",
    "for epoch in range(1):  # 데이터셋을 수차례 반복합니다.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i,(input_path,target_path) in enumerate(train_list):\n",
    "        input = PIL.Image.open(input_path).convert('L')\n",
    "        target = PIL.Image.open(target_path).convert('L')\n",
    "        input=trans(input).reshape(1,1,64,64)\n",
    "        print(input.shape)\n",
    "        target=trans(target).reshape(1,1,64,64)\n",
    "        optimizer.zero_grad()\n",
    "        #input, target = input.to(device), target.to(device)\n",
    "        #print(input_path)print(target_path)\n",
    "        output = net(input)\n",
    "        #ce_loss = torch.mean(torch.pow(output-target,2))\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i,(input_path,target_path) in enumerate(test_list):\n",
    "        input = PIL.Image.open(input_path)\n",
    "        target = PIL.Image.open(target_path)\n",
    "        input = trans(input).reshape(1, 1, 64, 64)\n",
    "        target = trans(target).reshape(1, 1, 64, 64)\n",
    "        #input, target = input.to(device), target.to(device)\n",
    "\n",
    "        output = net(input)\n",
    "        #loss = criterion(output, target)\n",
    "        #running_loss += loss.item()\n",
    "        #if i == len(test_list)-1:  # print every 2000 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' %\n",
    "        #          (epoch + 1, i + 1, running_loss / len(test_list)))\n",
    "        #    running_loss = 0.0\n",
    "        output1 = output.data\n",
    "        output1 = output1.reshape(1, 64, 64) * 255\n",
    "        output1[output1 > 255] = 255\n",
    "        img=trans_img(output1)\n",
    "        plt.imsave('./output/'+input_path[-9:],img)\n",
    "\n",
    "print('Finished Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'JpegImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-977dba2b6068>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m#mean1[mean1>255]=255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./output_layer5_best_2/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./output_layer5_best_2/in_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Finished Testing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimsave\u001b[1;34m(fname, arr, **kwargs)\u001b[0m\n\u001b[0;32m   2155\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_dedent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2157\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[0mFigureCanvas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m         fig.figimage(arr, cmap=cmap, vmin=vmin, vmax=vmax, origin=origin,\n\u001b[1;32m-> 1433\u001b[1;33m                      resize=True)\n\u001b[0m\u001b[0;32m   1434\u001b[0m         \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ParkHyeonGeun\\Anaconda3\\envs\\test\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mfigimage\u001b[1;34m(self, X, xo, yo, alpha, norm, cmap, vmin, vmax, origin, resize, **kwargs)\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dpi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m             \u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdpi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'JpegImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import PIL.Image\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "\n",
    "network_para = '../best_point.pth'\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, 1, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 9, 5, 1, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(9)\n",
    "        self.conv3 = nn.Conv2d(9, 15, 5, 1, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(15)\n",
    "        self.conv4 = nn.Conv2d(15, 5, 5, 1, 2)\n",
    "        self.bn4 = nn.BatchNorm2d(5)\n",
    "        self.conv5 = nn.Conv2d(5, 3, 5, 1, 2)\n",
    "        self.bn5 = nn.BatchNorm2d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(network_para))\n",
    "test_list = np.genfromtxt('./testset2.txt',dtype=None,encoding='UTF-8')\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "trans_img = transforms.ToPILImage()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net.to(device)\n",
    "\n",
    "try:\n",
    "    if not(os.path.isdir('./output_layer5_best_2/')):\n",
    "        os.makedirs(os.path.join('./output_layer5_best_2/'))\n",
    "except OSError as e:\n",
    "    if e.errno != e.errno.EEXIST:\n",
    "        print(\"Failed to create directory!!!!!\")\n",
    "        raise\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    for i,(input_path,target_path) in enumerate(test_list):\n",
    "        input = PIL.Image.open(input_path)\n",
    "        target = PIL.Image.open(target_path)\n",
    "        input = trans(input).reshape(1, 3, 64, 64)\n",
    "        target = trans(target).reshape(1, 3, 64, 64)\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        output = net(input)\n",
    "        loss = criterion(output, target)\n",
    "        running_loss += loss.item()\n",
    "        if i == len(test_list)-1:  # print every 2000 mini-batches\n",
    "            print('[%5d] loss: %.3f' %\n",
    "                  (i + 1, running_loss / len(test_list)))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "        output1 = (output.data).cpu()  # output1 tensor\n",
    "        #print(output1.shape)\n",
    "        output1 = (output1.reshape(3, 64, 64) * 255).numpy()\n",
    "        #print(output1.shape)\n",
    "        output1[output1>255] = 255\n",
    "        output1 = output1.astype(np.uint8)\n",
    "        #print(output1.shape)\n",
    "        mean1 = np.mean(output1, axis=0)\n",
    "        #mean1[mean1>255]=255\n",
    "        plt.imsave('./output_layer5_best_2/'+input_path[-9:],mean1,cmap=plt.get_cmap('gray'))\n",
    "\n",
    "print('Finished Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "[  487] loss: 0.121\n",
      "Finished Testing\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import PIL.Image\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "\n",
    "network_para = '../ver2_best_point.pth'\n",
    "save_path = '../ver2_output_best_2_final1/'\n",
    "\n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, 1, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 9, 5, 1, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(9)\n",
    "        self.conv3 = nn.Conv2d(9, 15, 5, 1, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(15)\n",
    "        self.conv4 = nn.Conv2d(15, 5, 5, 1, 2)\n",
    "        self.bn4 = nn.BatchNorm2d(5)\n",
    "        self.conv5 = nn.Conv2d(5, 3, 5, 1, 2)\n",
    "        self.bn5 = nn.BatchNorm2d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        return x\n",
    "''' # prev network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, 1, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 9, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(9)\n",
    "        self.conv3 = nn.Conv2d(9, 16, 5, 1, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        self.conv4 = nn.Conv2d(16, 32, 3, 1, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "        self.conv5 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.conv6 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        self.conv7 = nn.Conv2d(64, 32, 3, 1, 1)\n",
    "        self.bn7 = nn.BatchNorm2d(32)\n",
    "        self.conv8 = nn.Conv2d(32, 16, 5, 1, 2)\n",
    "        self.bn8 = nn.BatchNorm2d(16)\n",
    "        self.conv9 = nn.Conv2d(16, 8, 3, 1, 1)\n",
    "        self.bn9 = nn.BatchNorm2d(8)\n",
    "        self.conv10 = nn.Conv2d(8, 3, 5, 1, 2)\n",
    "        self.bn10 = nn.BatchNorm2d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = F.relu(self.bn7(self.conv7(x)))\n",
    "        x = F.relu(self.bn8(self.conv8(x)))\n",
    "        x = F.relu(self.bn9(self.conv9(x)))\n",
    "        x = F.relu(self.bn10(self.conv10(x)))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(network_para))\n",
    "test_list = np.genfromtxt('./testset.txt',dtype=None,encoding='UTF-8')\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "trans_img = transforms.ToPILImage()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net.to(device)\n",
    "\n",
    "try:\n",
    "    if not(os.path.isdir(save_path)):\n",
    "        os.makedirs(os.path.join(save_path))\n",
    "except OSError as e:\n",
    "    if e.errno != e.errno.EEXIST:\n",
    "        print(\"Failed to create directory!!!!!\")\n",
    "        raise\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    for i,(input_path,target_path) in enumerate(test_list):\n",
    "        input = PIL.Image.open(input_path)\n",
    "        target = PIL.Image.open(target_path)\n",
    "        input = trans(input).reshape(1, 3, 64, 64)\n",
    "        target = trans(target).reshape(1, 3, 64, 64)\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        output = net(input)\n",
    "        loss = criterion(output, target)\n",
    "        running_loss += loss.item()\n",
    "        if i == len(test_list)-1:  # print every 2000 mini-batches\n",
    "            print('[%5d] loss: %.3f' %\n",
    "                  (i + 1, running_loss / len(test_list)))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "        output1 = (output.data).cpu()  # output1 tensor\n",
    "        #print(output1.shape)\n",
    "        output1 = (output1.reshape(3, 64, 64) * 255).numpy()\n",
    "        #print(output1.shape)\n",
    "        output1[output1>255] = 255\n",
    "        output1 = output1.astype(np.uint8)\n",
    "        #print(output1.shape)\n",
    "        mean1 = np.mean(output1, axis=0)\n",
    "        #mean1[mean1>255]=255\n",
    "        plt.imsave(save_path+input_path[-9:],mean1,cmap=plt.get_cmap('gray'))\n",
    "\n",
    "print('Finished Testing')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
